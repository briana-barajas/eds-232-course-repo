---
title: "Lab5_Demo"
author: "Mateo Robbins"
date: "2024-02-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)    
library(ggplot2) #great plots
library(rsample)  #data splitting 
library(recipes) #data preprocessing
library(skimr) #data exploration
library(tidymodels) #re-entering tidymodel mode
library(kknn) #knn modeling
```

###k-nearest neighbor in tidymodels

## Data

```{r data}
data(attrition)
churn <- attrition %>% mutate_if(is.ordered, .funs = factor, ordered = F) %>% 
  janitor::clean_names()
#skim(churn) run in console
```

Not doing the data exploration here in the interest of time and since we are familiar with this dataset.

```{r initial_split}
set.seed(808)
#initial split of data, default 75/25
churn_split <- initial_split(churn)
churn_test <- testing(churn_split)
churn_train <- training(churn_split)
```

We need to create a recipe and do the preprocessing by dummy coding the nominal variables and standardizing the numeric variables.

```{r recipe}
#preprocessing
knn_rec <- recipe(attrition~., data = churn_train) %>% 
  step_dummy(all_nominal(), -all_outcomes(),
             one_hot = TRUE) %>% # type of dummy encoding
  step_normalize(all_numeric(), -all_outcomes()) %>% #
  prep()

baked_train <- bake(knn_rec, churn_train)
```

Recall: if you want to see the what the recipe is doing to your data, you can first prep() the recipe to estimate the parameters needed for each step and then bake(new_data = NULL) to pull out the training data with those steps applied.

Now the recipe is ready to be applied to the test data.

```{r bake_test}
baked_test
```

##Specify the k-nearest neighbor model

```{r knn_spec}

knn_spec <- nearest_neighbor(neighbors = 5) %>% 
  set_engine("kknn") %>% 
  set_mode("classification")
```

```{r knn_fit}
knn_fit <- knn_spec %>% 
  fit(attrition ~ ., data = churn_train)
```

```{r cv}
set.seed(808)

# 5-fold CV on the training dataset (instead of 10 for in-class demo)
cv_folds <- churn_train %>% 
  vfold_cv(v = 5) #set number of folds

```

Let's put it all together in a workflow.

```{r knn_workflow}
knn_workflow <- workflow() %>% 
  add_model(knn_spec) %>% 
  add_recipe(knn_rec)
```

Let's fit the resamples and carry out the cross-validation
```{r knn_res}
knn_res <- 
  knn_workflow %>% 
  fit_resamples(
    resamples = cv_folds,
    control = control_resamples(save_pred = TRUE)
  )
```

```{r}
# Check the performance
knn_res %>% 
  collect_metrics()
```
__Interpretation:__ `n` is the number of folds, the averages of all 5 folds are used to calculate accuracy and roc_auc. On average, the model has a 79% prediction accuracy (?)

Let's find the best value of k
```{r spec_with_tuning}
# Define our KNN model with tuning
knn_spec_tune <- nearest_neighbor(neighbors = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kknn")
  
```
__Note:__ Tuning the model to give the best model of the neighbors parameter. We arbitrarily selected 5, but it does not need to be the same as the number of folds. Additional, 10-fold is best practice but runs slower so we used 5 for this example.


```{r wf_knn_tune}
# Define a new workflow
wf_knn_tune <- workflow() %>% 
  add_model(knn_spec_tune) %>% 
  add_recipe(knn_rec)
```

This time before we fit the model we need to tell R which values to try for the parameter that we're tuning.

To tune our hyperparameter(s), we will use the tune_grid() function (instead of the fit() or fit_resamples() functions).

This tune_grid() is similar to fit_resamples() except that it takes an additional argument: grid. We will pass the possible values of our hyperparameter(s) to this grid argument, and it will evaluate each fold of our sample on each set of hyperparameters passed to grid.

We'll explore a few values of k: (1,5,10, 20, 30, 40, 50, 60, 70, 80, 90, 100)
```{r fit_knn_cv}
# Fit the workflow on our predefined folds and a grid of hyperparameters
fit_knn_cv <- wf_knn_tune %>% 
  tune_grid(
    cv_folds,
    grid = data.frame(neighbors = c(1, 5, seq(10, 100, 10))),
  )
  
# Check the performance with collect_metrics()
fit_knn_cv %>% collect_metrics()
```
__Note:__ This will run the model 60 times. There are 12 different values of neighbors we're trying, and each model is conducted using a 10-fold cv.

And finally, we will predict.

Use finalize_workflow() function wants (1) your initial workflow and (2) your best model.

```{r final_wf}
# The final workflow for our KNN model. Finalize_workflow takes a workflow and a set of parameters.  In this case, that set is just the best value of k
final_wf <- wf_knn_tune %>% 
  finalize_workflow(select_best(fit_knn_cv, # selects best model in terms of value for k
                                metric = "accuracy")) #use accuracy to select model 

# Check out the final workflow object.  Choosing accuracy for interpretability in this simple binary context
final_wf
```
__Interpretation:__ I think (??) this is saying the best value for k is 20.

```{r final_fit}
# Fitting our final workflow
final_fit <- final_wf %>% fit(data = churn_train)

# Examine the final workflow
final_fit
```
__Interpretation:__ Accuracy can be calculated buy subtracting the minimal misclassification from 1. 

And finally, we can predict onto the testing dataset.

```{r churn_pred}
# using final_fit, which is our best tuned model
churn_pred <- final_fit %>% 
  predict(new_data = churn_test)

# view results 
churn_pred %>% head()

```
__Interpretation:__ `final_fit` gives you a predicted value based on the outcome variable. In this case, it's predicted the first 6 individuals will not atrite. 

There's a better way! You can pass your final workflow (workflow plus the best model) to the last_fit() function along with your initial split (for us: churn_split) to both (a) fit your final model on your full training dataset and (b) make predictions onto the testing dataset (defined in your initial split object).

This last_fit() approach streamlines your work (combining steps) and also lets you easily collect metrics using the collect_metrics() function

```{r last_fit}
# Write over 'final_fit' with this last_fit() approach
final_fit <- final_wf %>% 
  last_fit(churn_split) # using full df in split form 

# Collect metrics on the test data!
final_fit %>% collect_metrics() 
```
__Interpretation:__ These are the metrics for this test are calculated using the test data (per documentation). Our final model has an accuracy of 84.2%.