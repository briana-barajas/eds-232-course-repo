---
title: "Lab 5"
author: "Briana Barajas"
date: "2023-02-07"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

This week's lab is a musical lab. You'll be requesting data from the Spotify API and using it to build k-nearest neighbor and decision tree models.

In order to use the Spotify API you must have a Spotify account. If you don't have one, sign up for a free one here: <https://www.spotify.com/us/signup>

Once you have an account, go to Spotify for developers (<https://developer.spotify.com/>) and log in. Click the green "Create a Client ID" button to fill out the form to create an app create an app so you can access the API.

On your developer dashboard page, click on the new app you just created. Go to Settings -\> Basic Information and you will find your Client ID . Click "View client secret" to access your secondary Client ID. Scroll down to Redirect URIs and enter: <http://localhost:1410/>

You have two options for completing this lab.

**Option 1**: **Classify by users**. Build models that predict whether a given song will be in your collection vs. a partner in class. This requires that you were already a Spotify user so you have enough data to work with. You will download your data from the Spotify API and then exchange with another member of class.

```{r, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r}
library(spotifyr) #API interaction
library(tidyverse)
library(tidymodels)
library(here)
library(baguette)

library(kableExtra) # tables and plots
library(gridExtra)

```

Client ID and Client Secret are required to create and access token that is required to interact with the API. You can set them as system values so we don't have to do provide them each time.

```{r access_API}

Sys.setenv(SPOTIFY_CLIENT_ID = '7bb9be2a18d640a6ac7972f39e8efc63') 
Sys.setenv(SPOTIFY_CLIENT_SECRET = 'd509f9cbeb8646bfa5ae41fce2f4056f')

authorization_code <- get_spotify_authorization_code(scope = scopes()[c(1:19)]) #sets an authorization code that you'll need to provide for certain get_ functions via 


access_token <- get_spotify_access_token() #takes ID and SECRET, sends to Spotify and receives an access token
```

**Option 1: Data Preparation**

You can use `get_my_saved_tracks()` to request all your liked tracks. It would be good if you had at least 150-200 liked tracks so the model has enough data to work with. If you don't have enough liked tracks, you can instead use `get_my_recently_played()`, and in that case grab at least 500 recently played tracks if you can.

The Spotify API returns a dataframe of tracks and associated attributes. However, it will only return up to 50 (or 20) tracks at a time, so you will have to make multiple requests. Use a function to combine all your requests in one call.

```{r}
# function to get last 200 likes
top_tracks_fun <- function(code){
  
  top_50 <- get_my_saved_tracks(limit = 50, offset = 0,
                                authorization = code)
  
  top_100 <- get_my_saved_tracks(limit = 50, offset = 50,
                                 authorization = code)
  
  top_150 <- get_my_saved_tracks(limit = 50, offset = 100,
                                 authorization = code)
  
  top_200 <- get_my_saved_tracks(limit = 50, offset = 150,
                                 authorization = code)
  
  rbind(top_50, top_100, top_150, top_200)
  
  
}

# use fun to store my liked song data
my_tracks <- top_tracks_fun(code = authorization_code)

```

Once you have your tracks, familiarize yourself with this initial dataframe. You'll need to request some additional information for the analysis. If you give the API a list of track IDs using `get_track_audio_features()`, it will return an audio features dataframe of all the tracks and some attributes of them.

These track audio features are the predictors we are interested in, but this dataframe doesn't have the actual names of the tracks. Append the 'track.name' column from your favorite tracks database.

```{r}
# create function that gets audio features for 500 liked songs
audio_features_fun <- function(row_seq){
  
  # get audio features for top 100 songs
  top_100 <- get_track_audio_features(my_tracks[c(1:row_seq), 8])
  
  top_200 <-  get_track_audio_features(my_tracks[c(101:200), 8])
  
  rbind(top_100, top_200)
}

# create df using function
my_tracks_audio <- audio_features_fun(row_seq = 100)

# isolate columns from my_tracks to join to audio tracks
my_tracks <- my_tracks %>% 
  select(track.id, track.name) %>% 
  rename(id = track.id)

# join dataframes so audio data has list of track.name
my_tracks_audio <- left_join(my_tracks_audio, my_tracks, by = "id")
```

Find a class mate whose data you would like to use. Add your partner's data to your dataset. Create a new column that will contain the outcome variable that you will try to predict. This variable should contain two values that represent if the track came from your data set or your partner's.

```{r}
## ..................save and send data...................
# save my data to send to partner (commented out for knitting)
#write_csv(my_tracks_audio, file = here("labs", "data", "briana_tracks_audio.csv"))

# load partners data
rosemary_tracks_audio <- read_csv(here("labs", "data", "rosemary_tracks_audio.csv"))

## ..................combine data...................
# create cols unique to myself and partner
my_tracks_audio$partner <- "Briana"
rosemary_tracks_audio$partner <- "Rosemary"

# bind and clean data frames
compare_track_audio <- rbind(my_tracks_audio, rosemary_tracks_audio) %>% 
  select(-c(type, id, uri, track_href, analysis_url, track.name)) %>% 
  mutate(time_signature = as.factor(time_signature),
         partner = as.factor(partner),
         mode = as.factor(mode),
         key = as.factor(key))

rm(top_tracks_fun, access_token, audio_features_fun, my_tracks, my_tracks_audio, rosemary_tracks_audio)
```

### Data Exploration

Let's take a look at your data. Do some exploratory summary stats and visualization.

For example: What are the most danceable tracks in your dataset? What are some differences in the data between users (Option 1) or genres (Option 2)?

```{r}
# create table of summary statistics for some variables
compare_track_audio %>% 
  group_by(partner) %>% 
  summarise(mean_dance = mean(danceability),
            mean_energy = mean(energy),
            mean_loudness = mean(loudness),
            max_loudness = max(loudness),
            min_acoustic = min(acousticness)) %>% 
  kbl(caption = "Summary Statistics") %>% 
  kable_minimal()

# visualize some of these differences
grid.arrange(
  
(compare_track_audio %>% 
  filter(key %in% c("4", "5", "6", "7", "8", "9", "10")) %>%
  ggplot(aes(x = key, fill = partner)) +
  geom_bar(stat = "count", position = "dodge") +
  theme_minimal() +
  scale_fill_manual(values = c("lightskyblue", "palevioletred")) +
  labs(x = "Key", y = "Count", fill = "Partner")),

(compare_track_audio %>% 
  ggplot(aes(x = duration_ms, fill = partner)) +
  geom_histogram(alpha = 0.6, position = "identity") +
  theme_minimal() +
  scale_fill_manual(values = c("lightskyblue", "palevioletred")) +
  labs(x = "Duration (ms)", y = "Count", fill = "Partner")),

ncol = 1)
```

### **Modeling**

Create competing models that predict whether a track belongs to:

Option 1. you or your partner's collection

You will eventually create four final candidate models:

```{r}
## ==================================
##    Pre-processing ALL Models  ----
## ==================================
# set seed and split data
set.seed(125)

# split data
audio_split <- initial_split(compare_track_audio, prop = 0.7)
audio_train <- training(audio_split)
audio_test <- testing(audio_split)

# create recipe
recip <- recipe(partner~., data = audio_train) %>% 
  step_dummy(all_nominal(), -all_outcomes(),
             one_hot = TRUE) %>% 
  step_normalize(all_numeric(), -all_outcomes()) %>% 
  prep()

# set number of cross validation folds
cv_folds <- vfold_cv(audio_train, v = 10)

```

### k-nearest neighbors (Week 5)

```{r knn}
set.seed(125)

## ==================================
##        KNN Pre-processing    ----
## ==================================

# specify KNN as the model
# Define nearest neighbor model (EMD)
knn_spec <- nearest_neighbor(neighbors = 5) %>% 
  set_engine("kknn") %>% 
  set_mode("classification")

## ==================================
##            KNN Resample       ----
## ==================================

# combine into workflow
# Workflow (EMD)
knn_workflow <- workflow() %>% 
  add_model(knn_spec) %>%
  add_recipe(recip)

# fit resamples
# Fit resamples (EMD)
knn_resamples <- knn_workflow %>% 
  fit_resamples(
    resamples = cv_folds,
    control = control_resamples(save_pred = TRUE)
  )

## ==================================
##             KNN Tuning        ----
## ==================================

# define KNN model with tuning
# Tune the hyperparameters (EMD)
knn_spec_tune <- nearest_neighbor(neighbors = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kknn")

# define new wf with tuned hyperparameters
# Workflow: Define new workflow (EMD)
knn_wf_tuned <- workflow() %>% 
  add_model(knn_spec_tune) %>% 
  add_recipe(recip)

# fit the workflow on our pre-defined folds hyperparameters
# Fit workflow on predefined folds and hyperparameters (EMD)
fit_knn_cv <- knn_wf_tuned %>% 
  tune_grid(
    cv_folds,
    grid = data.frame(neighbors = c(1, 5, seq(10, 100, 10))))

## ==================================
##          KNN Final Model      ----
## ==================================

# Workflow: Final (EMD)
knn_final_wf <- knn_wf_tuned %>% 
  finalize_workflow(select_best(fit_knn_cv, 
                                metric = "accuracy"))

# fit the final work flow
# Fit: Final (EMD)
knn_final_fit <- knn_final_wf %>% fit(data = audio_train)
knn_pred <- knn_final_fit %>% predict(new_data = audio_test)

knn_final_fit <- knn_final_fit %>% last_fit(audio_split)

# View final results
knn_final_fit %>% collect_metrics()

# store metrics
knn_metrics <- as.data.frame(knn_final_fit$.metrics)

# clean environment 
rm(knn_spec, knn_workflow, knn_resamples, knn_spec_tune,
   knn_wf_tuned, fit_knn_cv, knn_final_wf, knn_final_fit)
```

### Decision Tree (Week 5)

```{r decision-tree}

set.seed(125)

## ==================================
##  Decision Tree Pre-processing ----
## ==================================

# set-up a decision tree specification
tree_spec_fixed <- decision_tree(
  cost_complexity = 0.1,
  tree_depth = 4,
  min_n = 11) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

## ==================================
##      Decision Tree Tuning     ----
## ==================================

# tell the model that we are tuning hyperameters
tree_spec_tune <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

# create grid
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          min_n(), levels = 5)

# create workflow
wf_dt_tune <- workflow() %>% 
  add_recipe(recip) %>% 
  add_model(tree_spec_tune)

## ==================================
##    Decision Tree Resampling   ----
## ==================================

# build decision trees in parallel
doParallel::registerDoParallel() 

dt_tree_rs <- tune_grid(
    wf_dt_tune,
    resamples = cv_folds,
    grid = tree_grid,
    metrics = metric_set(accuracy)) #END tune_grid

# select best hyperparameters
final_dt <- finalize_workflow(wf_dt_tune,
                                  select_best(dt_tree_rs))

## ==================================
##          DT Final Model       ----
## ==================================
dt_final_fit <- fit(final_dt, data = audio_train)

dt_final_result <- last_fit(dt_final_fit, audio_split)

# store final results as df
dt_pred <- as.data.frame(final_dt_result$.predictions) %>% 
  bind_cols(audio_test)

# store metrics
dt_metrics <- as.data.frame(dt_final_result$.metrics)

# clear environment
rm(tree_spec_fixed, tree_spec_tune, tree_grid, wf_dt_tune,
   dt_tree_rs, final_dt, dt_final_fit, dt_final_result)
```

### Bagged Tree (Week 6)

```{r bagged-tree}

set.seed(125)

## ==================================
##         Bagged Tree Tuning    ----
## ==================================

# tuning, 50 bagged trees
bt_spec_tune <- bag_tree(
  mode = "classification",
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()) %>% 
  set_engine("rpart", times = 50)

# define tree grid
bt_tree_grid <- grid_regular(cost_complexity(),
                             tree_depth(),
                             min_n(),
                             levels = 5)

# define workflow
bt_wf_tune <- workflow() %>% 
  add_recipe(recip) %>% 
  add_model(bt_spec_tune)

## ==================================
##      Bagged Tree Resampling   ----
## ==================================

# build model & fit
doParallel::registerDoParallel()

bt_tree_rs <- bt_wf_tune %>% 
  tune_grid(partner~.,
            resamples = cv_folds,
            grid = bt_tree_grid,
            metrics = metric_set(accuracy))

## ==================================
##          BT Final Model       ----
## ==================================
# finalize tuned model
bt_final <- finalize_workflow(bt_wf_tune, select_best(bt_tree_rs, "accuracy")) %>% 
  fit(data = audio_train)

# make predictions
bt_pred <- bt_final %>% 
  predict(new_data = audio_test) %>% 
  bind_cols(audio_test)

# save metrics
bt_metrics <- bt_pred %>% 
  metrics(truth = partner, estimate = .pred_class)

rm(bt_final, bt_tree_rs, bt_wf_tune, bt_tree_grid, bt_spec_tune)
```

```{r random-forest}

set.seed(125)

## ==================================
##      Random Forest Tuning     ----
## ==================================

# tune model
rf_model <- rand_forest(mtry = tune(),
                        min_n = tune(),
                        trees = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

# define workflow
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(recip)

## ==================================
##    Random Forest Tuning       ----
## ==================================

# tune hyperparameters
rf_cv_tune <- rf_workflow %>% 
  tune_grid(resamples = cv_folds, grid = 10)

## ==================================
##      Random Forest Final      ----
## ==================================
# define best 
rf_best <- show_best(rf_cv_tune, n = 1,
                     metric = "roc_auc")

# finalize workflow
rf_final <- finalize_workflow(rf_workflow,
                              select_best(rf_cv_tune, metric = "roc_auc"))

# fit the model to the training set
train_fit_rf <- fit(rf_final, audio_train)

# get predictions
rf_pred <- predict(train_fit_rf, audio_test) %>% 
  bind_cols(audio_test)

# get accuracy of testing prediction
rf_metrics <- as.data.frame(accuracy(rf_pred, truth = partner, estimate = .pred_class))


rm(rf_model, rf_workflow, rf_cv_tune, rf_best, rf_final, train_fit_rf)
```

1.  k-nearest neighbor (Week 5)
2.  decision tree (Week 5)
3.  bagged tree (Week 6)

-   bag_tree()

-   Use the "times =" argument when setting the engine during model specification to specify the number of trees. The rule of thumb is that 50-500 trees is usually sufficient. The bottom of that range should be sufficient here. (**NOTE**: Will use this function in

    `min_n = tune ()), %>% set_engine("rpart", times=50) %>% set_mode("classification"`

    do NOT set the mode within the `bag_trees()` function, any extra control variables can go within `set_engine()`). There are extra variables from the model function documentation that won't appear in the documentation for `set_engine()` . **Chunk of code from answer key** üê≠

4.  random forest (Week 6)

-   rand_forest()
-   `m_try()` is the new hyperparameter of interest for this type of model. Make sure to include it in your tuning process

Go through the modeling process for each model:

Preprocessing. You can use the same recipe for all the models you create.

Resampling. Make sure to use appropriate resampling to select the best version created by each algorithm.

Tuning. Find the best values for each hyperparameter (within a reasonable range).

Compare the performance of the four final models you have created.

Use appropriate performance evaluation metric(s) for this classification task. A table would be a good way to display your comparison. Use at least one visualization illustrating your model results.
