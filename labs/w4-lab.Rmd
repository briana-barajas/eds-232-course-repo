---
title: "Lab 4"
author: "Briana Barajas"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(skimr)
library(tidymodels)
library(caret)
library(recipes)
library(corrplot)
```

## Lab 4: Fire and Tree Mortality

The database we'll be working with today includes 36066 observations of individual trees involved in prescribed fires and wildfires occurring over 35 years, from 1981 to 2016. It is a subset of a larger fire and tree mortality database from the US Forest Service (see data description for the full database here: [link](https://www.nature.com/articles/s41597-020-0522-7#Sec10)). Our goal today is to predict the likelihood of tree mortality after a fire.

### Data Exploration

Outcome variable: *yr1status* = tree status (0=alive, 1=dead) assessed one year post-fire.

Predictors: *YrFireName, Species, Genus_species, DBH_cm, CVS_percent, BCHM_m, BTL* (Information on these variables available in the database metadata ([link](https://www.fs.usda.gov/rds/archive/products/RDS-2020-0001-2/_metadata_RDS-2020-0001-2.html))).

```{r, results='hide'}
trees_dat<- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/trees-dat.csv") %>% 
  select(c(yr1status, YrFireName, Species, Genus_species, DBH_cm, CVS_percent, BCHM_m, BTL)) %>% 
  janitor::clean_names()
```

> Question 1: Recode all the predictors to a zero_based integer form

```{r}
# define recode using recipe
trees_recip <- recipe(yr1status~., data = trees_dat) %>% 
  step_integer(all_predictors(), zero_based = TRUE) %>% 
  prep(trees_dat)

# apply step_integer through bake
trees_bake <- bake(trees_recip, new_data = trees_dat)

```

### Data Splitting

> Question 2: Create `trees_training` (70%) and `trees_test` (30%) splits for the modeling

```{r}
set.seed(123)

# split baked tree data
trees_split <- initial_split(trees_bake, prop = 0.7)

# isolate training and test data
trees_training <- training(trees_split)
trees_test <- testing(trees_split)

```

> Question 3: How many observations are we using for training with this split?

```{r}
glue::glue("Number of observations in training data: {nrow(trees_training)}")
```

### Simple Logistic Regression

Let's start our modeling effort with some simple models: one predictor and one outcome each.

> Question 4: Choose the three predictors that most highly correlate with our outcome variable for further investigation.

```{r}
# summarize all correlations to select most correlated variables
trees_bake %>% 
  cor() %>% 
  corrplot(method = "shade", shade.col = NA, 
           tl.col = "black", tl.srt = 45, 
           addCoef.col = "black", cl.pos = "n", 
           order = "original")

```

**ANS:** Based on the correlation plot, the three variables that are most highly correlated to tree survival (`yr1status`) are `dbh_cm`, `cvs_percent`, and `bchm_m`. These correspond to diameter at breast height, percent of pre-fire crown volume burned, and maximum bark char, respectively.

> Question 5: Use glm() to fit three simple logistic regression models, one for each of the predictors you identified.

```{r}
# glm for dbh
model_dbh <- glm(data = trees_bake,
                 yr1status~dbh_cm,
                 family = 'binomial')

# glm for cvs
model_cvs <- glm(data = trees_bake,
                 yr1status~cvs_percent,
                 family = 'binomial')

# glm for bchm
model_bchm <- glm(data = trees_bake,
                  yr1status~bchm_m,
                  family = 'binomial')
```

### Interpret the Coefficients

We aren't always interested in or able to interpret the model coefficients in a machine learning task. Often predictive accuracy is all we care about.

> Question 6: That said, take a stab at interpreting our model coefficients now.

```{r}
# print dbh coefficients
exp(coef(model_dbh))

# print cvs coefficients
exp(coef(model_cvs))

# print bchm
exp(coef(model_bchm))
```

**ANS:**

-   For every 1cm increase in diameter at breast height (`dbh_cm`) the odds of a tree dying 1 year post-fire decreases by 0.996.
-   For every 1% increase in the percent of pre-fire crown volume that was burned (`cvs_percent`), the odds of a tree dying 1 year post-fire increases by 1.08.
-   For every 1m increase in average bark char (`bchm_m`), the odds of a tree dying 1 year post-fire increases by 1.00.

> Question 7: Now let's visualize the results from these models. Plot the fit to the training data of each model.

```{r, message=FALSE}
gridExtra::grid.arrange(
  
  # plot dbh variable
  (ggplot(trees_bake, aes(x = dbh_cm, y = yr1status)) +
     geom_point() +
     stat_smooth(method = 'glm', se = TRUE, 
                 method.args = list(family = 'binomial')) +
     theme_minimal()),
  
  # plot cvs variable
  (ggplot(trees_bake, aes(x = cvs_percent, y = yr1status)) +
     geom_point() +
     stat_smooth(method = 'glm', se = TRUE, 
                 method.args = list(family = 'binomial')) +
     theme_minimal()),
  
  # plot bark char variable
  (ggplot(trees_bake, aes(x = bchm_m, y = yr1status)) +
     geom_point() +
     stat_smooth(method = 'glm', se = TRUE, 
                 method.args = list(family = 'binomial')) +
     theme_minimal()),
  
  nrow = 2
)
```


### Multiple Logistic Regression

Let's not limit ourselves to a single-predictor model. More predictors might lead to better model performance.

> Question 8: Use `glm()` to fit a multiple logistic regression called `logistic_full`, with all three of the predictors included. Which of these are significant in the resulting model?

```{r}
# create multiple logisitc regression
logistic_full <- glm(yr1status ~ dbh_cm + cvs_percent + bchm_m,
                     data = trees_bake, family = 'binomial')

# view model coefficients
tidy(logistic_full)
```
__ANS:__ The p-values for all three variables of interest are very small (close to zero). This means that `dbh_cm`, `cvs_percent`, and `bchm_m` are all significant variables in the multiple logistic regression. 

### Estimate Model Accuracy

Now we want to estimate our model's generalizability using resampling.

> Question 9: Use cross validation to assess model accuracy. Use `caret::train()` to fit four 10-fold cross-validated models (cv_model1, cv_model2, cv_model3, cv_model4) that correspond to each of the four models we've fit so far: three simple logistic regression models corresponding to each of the three key predictors (CVS_percent, DBH_cm, BCHM_m) and a multiple logistic regression model that combines all three predictors.

```{r}
# convert yr1status to a factor
trees_bake_fct <- trees_bake %>%
  mutate(yr1status = as_factor(yr1status))

# split data
trees_split_fct <- initial_split(trees_bake_fct, prop = 0.7)
trees_train_fct <- training(trees_split_fct)
trees_test_fct <- testing(trees_split_fct)

# run model yr1status + dhb_cm
cv_model1 <- train(
  yr1status ~ dbh_cm,
  data = trees_train_fct,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

# run model yr1status + cvs_percent
cv_model2 <- train(
  yr1status ~ cvs_percent,
  data = trees_train_fct,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

# run model yr1status + bchm_m
cv_model3 <- train(
  yr1status ~ bchm_m,
  data = trees_train_fct,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

# run multiple regression
cv_model4 <- train(
  yr1status ~ dbh_cm + cvs_percent + bchm_m,
  data = trees_train_fct,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

```


> Question 10: Use caret::resamples() to extract then compare the classification accuracy for each model. (Hint: resamples() wont give you what you need unless you convert the outcome variable to factor form). Which model has the highest accuracy?

Let's move forward with this single most accurate model.

```{r}
summary(
  resamples(
    list(
      model1 = cv_model1,
      model2 = cv_model2,
      model3 = cv_model3,
      model4 = cv_model4
    )
  )
)$statistics$Accuracy
```

__ANS:__ The mean model accuracy is calculated by taking the mean accuracy of all 10 folds. The `cv_model4`, which is the multiple linear regression, has the highest accuracy of all the models tested.

> Question 11: Compute the confusion matrix and overall fraction of correct predictions by the model.

```{r}
# predict class for the model with the highest accuracy
pred_class_trees <- predict(cv_model4, trees_train_fct)

# create confusion matrix
confusionMatrix(
  data = relevel(pred_class_trees, ref = "1" ), 
  reference = relevel(trees_train_fct$yr1status, ref = "1")
)
```

> Question 12: Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

__ANS:__ The confusion matrix displays information on the individuals that were in the data that were guessed correctly and incorrectly. The intersect between 1x1 is 6300, meaning that there were 6300 trees that were predicted to be dead and were actually dead. These values are the true positives, the true negatives would be the 16504 that were predicted to be alive, and were in fact alive. The remaining values (1595 and 847) represent the false positive and negatives. These were individual trees that the model guessed incorrectly. 

> Question 13: What is the overall accuracy of the model? How is this calculated?

__ANS:__ When assessing accuracy, it can be useful to compare the accuracy to the "no information rate." This rate demonstrates how accurate the model would be if it predicted that every tree survived (`yr1status` = 0). To assess the overall accuracy, the no information rate is compared to the baseline accuracy of 0.9. Once the no information rate is taken into account, the balanced accuracy for the model is 0.89 (slightly lower than before).

### Test Final Model

Alright, now we'll take our most accurate model and make predictions on some unseen data (the test data).

> Question 14: Now that we have identified our best model, evaluate it by running a prediction on the test data, trees_test.

```{r}
# predict using the multiple regression model + testing data
predict_trees_test <- predict(cv_model4, trees_test_fct)

# create confusion matrix for test
confusionMatrix(
  data = relevel(predict_trees_test, ref = "1"),
  reference = relevel(trees_test_fct$yr1status, ref = "1")
)
```


> Question 15: How does the accuracy of this final model on the test data compare to its cross validation accuracy? Do you find this to be surprising? Why or why not?

__ANS:__ The accuracy of the final model produces roughly the same accuracy as it did for the cross-validation using the training data. While the accuracy is lower on the test data, it is not as drastic as I would have expected. I would like to better asses how the no information rate supports or increases the model's accuracy.

