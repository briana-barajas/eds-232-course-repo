---
title: "Lab 8"
author: "Briana Barajas"
date: "2024-03-06"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

## Forest Cover Classification with SVM

```{r, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r, results='hide'}
library(tidyverse)
library(tidyselect)
library(tidymodels)
library(kernlab)
library(here)
library(gt)
```

In this week's lab we are exploring the use of Support Vector Machines for multi-class classification. Specifically, you will be using cartographic variables to predict forest cover type (7 types).

Natural resource managers responsible for developing ecosystem management strategies require basic descriptive information including inventory data like forest cover type for forested lands to support their decision-making processes. However, managers generally do not have this type of data for in-holdings or neighboring lands that are outside their immediate jurisdiction. One method of obtaining this information is through the use of predictive models.

You task is build both an SVM and a random forest model and compare their performance on accuracy and computation time.

1.  The data is available here: <https://ucsb.box.com/s/ai5ost029enlguqyyn04bnlfaqmp8kn4>

```{r, results='hide'}
# read in data
cover_type <- read_csv(here("labs", "data", "covtype_sample.csv")) %>% 
  mutate(Cover_Type = as.factor(Cover_Type))
```

Explore the data.

-   What kinds of features are we working with?

-   Does anything stand out that will affect you modeling choices?

Hint: Pay special attention to the distribution of the outcome variable across the classes.

```{r}
# view data (commented out)
# head(cover_type, 5) %>% gt()

# view column names (commented out)
#colnames(cover_type)

# count and plot number of cover types in data
plyr::count(cover_type$Cover_Type) %>% 
  mutate(x = as.factor(x)) %>% 
  
  ggplot(aes(x = x, y = freq)) +
  geom_col(fill = "darkmagenta") +
  labs(x = "Cover Type", y = "Frequency", title = "Cover Type Frequency") +
  theme_minimal() + theme(plot.title = element_text(hjust = 0.5, size = 16))
```

**ANS:** It appears the soil types are mutually exclusive. The data set could be pivoted to change this into a single column, although I know from previous experience that this should not affect the outcome. Soil type 15 has no entries, and this lack of variation was addressed in the following recipe stepped. Additionally, I noticed that cover type 1 and 2 are the most represented in the data. To diminish the effect of this uneven distribution, it's important to create stratified samples.

2.  Create the recipe and carry out any necessary preprocessing. Can you use the same recipe for both models?

```{r}
set.seed(125)

# split data
split <- initial_split(cover_type)
train <- training(split)
test <- testing(split)

# specify recipe for random forest
recip <- recipe(Cover_Type~., data = train) %>% 
  step_zv(all_predictors()) %>% #remove cols w/no variance
  step_center(all_predictors()) %>% #normalize
  step_scale(all_predictors())

##========================================
##           SVM Pre-processing       ----
##========================================

# create specification for 
svm_rbf_spec <- svm_rbf() %>% 
  set_mode("classification") %>% 
  set_engine("kernlab")

##========================================
##            RF Pre-processing       ----
##========================================

# create rf model specification
rf_spec <- rand_forest(mtry = tune(),
                       trees = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

# create rf model workflow
rf_workflow <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(recip)

```

**ANS:** Only one recipe is required for the random forest model. For the support vector machine, `svm_rbf()` will be used instead, and the recipe will be specified during the fit step. Since all the variables being tested are numeric, it's not necessary to create dummy variables with `step_dummy()`. To ignore variables such as soil type 15 that have no variation, `step_zv()` was applied first.

3.  Create the folds for cross-validation.

```{r}
# create cross validation with strata for cover type (account for uneven distribution)
cv_folds <- vfold_cv(data = train, v = 5, strata = Cover_Type)
```

4.  Tune the models. Choose appropriate parameters and grids. If the computational costs of tuning given your strategy are prohibitive, how might you work around this?

```{r}
##========================================
##              RF Tuning             ----
##========================================
# tune grid for rf
system.time( tune_res <- tune_grid(
  rf_workflow,
  resamples = cv_folds,
  grid = 5))

# # plot results
# autoplot(tune_res)

# collect metrics for best model
rf_best <- select_best(tune_res, metric = "roc_auc") 

# finalize workflow
rf_final <- finalize_workflow(rf_workflow, rf_best)
```

**ANS:** To avoid the high computation time for `tune_grid` in the svm model, I used `svm_rbf()` instead of `svm_poly()`. I was more limited when it came to the random forest model. I moved my repository to the Tsosie server, and decreased the number of folds and grid to 5 instead of 10.

5.  Conduct final predictions for both models and compare their prediction performances and computation costs from part 4.

-   Which type of model do you think is better for this task?
-   Why do you speculate this is the case?

```{r}
set.seed(125)

##========================================
##           SVM Final Predictions    ----
##========================================
# final fit svm
system.time( svm_rbf_fit <- svm_rbf_spec %>%
  fit(Cover_Type~., data = train) )

# make final predictions and bind with test data
svm_predict <- augment(svm_rbf_fit, new_data = test) 

##========================================
##           RF Final Predictions     ----
##========================================
# fit model
train_fit_rf <- fit(rf_final, train)

# make final predictions and bind with test data
rf_predict <- augment(train_fit_rf, new_data = test) 

##========================================
##            Compare Models          ----
##========================================
# view accuracy for each model 
svm_accuracy <- accuracy(svm_predict, truth = Cover_Type, estimate = .pred_class)
svm_accuracy$model <- "SVM"

rf_accuracy <- accuracy(rf_predict, truth = Cover_Type, estimate = .pred_class)
rf_accuracy$model <- "RF"

svm_accuracy %>% bind_rows(rf_accuracy) %>% gt() %>% 
  tab_header(title = "Model Accuracy")

```

**ANS:** Overall, the random forest model was the better option. Although using `svm_rbf()` decreased the computation time of the support vector machine model, it was still more than double the run time of the random forest. I will note that I decreased the grid, and number of folds to improve the random forest run time. Despite this, it was still the better model. Ideally, I would increase the number of folds to improve the random forest model. While very sophisticated, I suspect working in multiple dimensions and transforming data could have decreased the accuracy of the svm model.
