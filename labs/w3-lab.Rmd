---
title: "Lab 3 - Abalone Age"
author: "Briana Barajas"
date: 2024-01-25
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(glmnet)
```

## Lab 3: Predicting the age of abalone

Abalones are marine snails. Their flesh is widely considered to be a desirable food, and is consumed raw or cooked by a variety of cultures. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age.

The data set provided includes variables related to the sex, physical dimensions of the shell, and various weight measurements, along with the number of rings in the shell. Number of rings is the stand-in here for age.

### Data Exploration

Pull the abalone data from Github and take a look at it.

```{r data, warning=FALSE, message=FALSE}
# read in data
abdat<- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/abalone-data.csv") %>% 
  janitor::clean_names()

# View data summary 
glimpse(abdat)

```

### Data Splitting

-   ***Question 1***. Split the data into training and test sets. Use a 70/30 training/test split.
```{r}
# set seed for reproducability
set.seed(123)

# split data 70/30 
split <- initial_split(abdat, prop = 0.7)
ab_train <- training(split)
ab_test <- testing(split)
```

We'll follow our text book's lead and use the `caret` package in our approach to this task. We will use the `glmnet` package in order to perform ridge regression and the lasso. The main function in this package is `glmnet()`, which can be used to fit ridge regression models, lasso models, and more. In particular, we must pass in an x matrix of predictors as well as a y outcome vector , and we do not use the yâˆ¼x syntax.

### Fit a ridge regression model

-   ***Question 2***. Use the model.matrix() function to create a predictor matrix, x, and assign the Rings variable to an outcome vector, y.

```{r, message=FALSE}
# create model matrix
ab_X <- model.matrix(rings ~ ., ab_train)[,-1]

# assess if Y needs to be log transformed
ggplot(abdat) +
  geom_histogram(aes(x = rings), fill = '#CDB4DB', alpha = 0.7) +
  geom_histogram(aes(x = log(rings)), fill = '#A2D2FF', alpha = 0.7) +
  labs(caption = "Purple - no transformation\n Blue - log transformation",
       title = 'Ring Distribution') +
  theme_minimal()

# assign outcome vector Y, no transformation needed
ab_Y <- ab_train$rings

```

-   ***Question 3***. Fit a ridge model (controlled by the alpha parameter) using the glmnet() function. Make a plot showing how the estimated coefficients change with lambda. (Hint: You can call plot() directly on the glmnet() objects).
```{r}
# fit and plot the ridge model
plot(glmnet(x = ab_X, y = ab_Y,
            alpha = 0))
```


### Using *k*-fold cross validation resampling and tuning our models

In lecture we learned about two methods of estimating our model's generalization error by resampling, cross validation and bootstrapping. We'll use the *k*-fold cross validation method in this lab. Recall that lambda is a tuning parameter that helps keep our model from over-fitting to the training data. Tuning is the process of finding the optima value of lamba.

-   ***Question 4***. This time fit a ridge regression model and a lasso model, both with using cross validation. The `glmnet` package kindly provides a `cv.glmnet()` function to do this (similar to the `glmnet()` function that we just used). Use the alpha argument to control which type of model you are running. Plot the results.

```{r}
# apply k-fold ridge regression
ab_ridge <- cv.glmnet(x = ab_X, y = ab_Y,
                      alpha = 0)

# apply k-fold lasso model
ab_lasso <- cv.glmnet(x = ab_X, y = ab_Y,
                      alpha = 1)

# Plot results
par(mfrow = c(1,2))
plot(ab_ridge, main = "Ridge Penalty")
plot(ab_lasso, main = "Lasso Penalty")
```


-   ***Question 5***. Interpret the graphs. What is being displayed on the axes here? How does the performance of the models change with the value of lambda?
__ANS:__ The x-axis represents the logged value of lambda, and the y-axis is the mean-squared error (MSE) for the model. For both the ridge and lasso regression increasing the penalty also increases the MSE, meaning the models with higher constraints are less suitable.

-   ***Question 6***. Inspect the _ridge_ model object you created with `cv.glmnet()`. The \$cvm column shows the MSEs for each CV fold. What is the minimum MSE? What is the value of lambda associated with this MSE 
minimum?

```{r}
# inspect ridge model
glue::glue("Inspect ridge model \n Minimum MSE for a cv-fold: {min(ab_ridge$cvm)}\n Lambda for min cv-fold: {ab_ridge$lambda.min}\n")

```


-   ***Question 7***. Do the same for the lasso model. What is the minimum MSE? What is the value of lambda associated with this MSE minimum?

```{r}
glue::glue("Inspect lasso model \n Minimum MSE for a cv-fold: {min(ab_lasso$cvm)}\n Lambda for min cv-fold: {ab_lasso$lambda.min}\n")
```

Data scientists often use the "one-standard-error" rule when tuning lambda to select the best model. This rule tells us to pick the most parsimonious model (fewest number of predictors) while still remaining within one standard error of the overall minimum cross validation error. The `cv.glmnet()` model object has a column that automatically finds the value of lambda associated with the model that produces an MSE that is one standard error from the MSE minimum (\$lambda.1se).

-   ***Question 8.*** Find the number of predictors associated with this model (hint: the \$nzero is the \# of predictors column).
```{r}
# find number of coefficients for 1-SE lasso model
glue::glue("Number of predictors for the lasso model that follows the 1-SE rule: {ab_lasso$nzero[ab_lasso$lambda == ab_lasso$lambda.1se]} ")
```


-   ***Question 9*****.** Which regularized regression worked better for this task, ridge or lasso? Explain your answer.

__ANS:__ The lasso model has a smaller MSE (4.75 < 5.06), meaning it was able to predict abalone rings based on the additional variables with less error than the ridge model. Within the lasso model, you must also select the model with the minimum MSE, or the model within one standard error of the MSE. In this case, the lasso model within one standard error of the lowest MSE allows you to efficiently predict abalone rings without measuring for all 10 variables. This can have important implications for the data collection process since less variables can be measured. 


